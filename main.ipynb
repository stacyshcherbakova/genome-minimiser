{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dir import *\n",
    "from VAE_model import *\n",
    "from VAE_model_single import *\n",
    "from VAE_MoG_model import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Overall exporation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(PANGENOME_MATRIX_CSV, index_col=[0], header=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.transpose()[data.transpose()[data.transpose().columns].eq(0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_GF_present = data.astype(bool).sum(axis=0) / len(data.index) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_GF_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "percent_GF_present.iloc[:100].plot(kind='bar')\n",
    "plt.xlabel('Genomes')\n",
    "plt.ylabel('Percentage of GFs present in the genome')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency1 = data.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(frequency1)\n",
    "plt.xlabel('Gene count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency2 = data.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,8))\n",
    "# plt.hist(frequency2, bin=20)\n",
    "# plt.xlabel('Genome size')\n",
    "# plt.ylabel('Gene Gamily Frequency')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_data = []\n",
    "thresholds = np.linspace(0, 20, num=10)\n",
    "\n",
    "for i in thresholds:\n",
    "    row_sums = data.sum(axis=1)\n",
    "    threshold_data.append(len(data[row_sums >= i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(thresholds, threshold_data)\n",
    "plt.plot(thresholds, threshold_data)\n",
    "plt.xlabel('Gene Number Thershold')\n",
    "plt.ylabel('Gene Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(data.transpose())\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Visualize the first two principal components\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='PC1', y='PC2', data=df_pca)\n",
    "plt.title('PCA')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import shapiro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test_pc1 = shapiro(df_pca['PC1'])\n",
    "shapiro_test_pc2 = shapiro(df_pca['PC2'])\n",
    "print(f\"Shapiro-Wilk Test for PC1: {shapiro_test_pc1}\")\n",
    "print(f\"Shapiro-Wilk Test for PC2: {shapiro_test_pc2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data preprocessing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = data.sum(axis=1)\n",
    "filtered_data = data[row_sums >= 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t = np.array(filtered_data.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalizing the data with Standard Scaler\n",
    "# scaler = StandardScaler()\n",
    "# data_normalized = scaler.fit_transform(data_array_t)\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data_tensor = torch.tensor(data_array_t, dtype=torch.float32)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_data, val_data = train_test_split(data_tensor, test_size=0.2, random_state=12345)\n",
    "\n",
    "# TensorDataset\n",
    "train_dataset = TensorDataset(train_data)\n",
    "val_dataset = TensorDataset(val_data)\n",
    "\n",
    "# DataLoaders for main training\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Dataloader for overfitting on one sample (for dubbiging purposes)\n",
    "input_dim = data_array_t.shape[1]\n",
    "binary_data = torch.tensor(np.random.randint(0, 2, size=(1, input_dim)), dtype=torch.float32)\n",
    "single_sample_dataset = TensorDataset(binary_data)\n",
    "single_sample_loader = DataLoader(single_sample_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Dataloader fot a small subset for overfitting (again, for debugging)\n",
    "small_subset_indices = np.random.choice(len(train_dataset), size=256, replace=False)\n",
    "small_subset = Subset(train_dataset, small_subset_indices)\n",
    "small_loader = DataLoader(small_subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorDataset(torch.tensor(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Overfitting on a single sample and small data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Overfitting on a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample = torch.randn(1, data_array_t.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO GRADIENT CLIPPING AND SCHEDULER \n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "free_bits = 0.1\n",
    "n_epochs = 10\n",
    "\n",
    "model = VAE_single(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Overfitting one sample to see if the model is broken \n",
    "model.train()\n",
    "num_epochs = 1000\n",
    "\n",
    "# Collecting data for visualisation \n",
    "train_loss_vals1 = []\n",
    "train_loss_vals2 = []\n",
    "kl_divergences_no_beta = []\n",
    "kl_divergences_beta = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    beta = beta_start + (beta_end - beta_start) * epoch / n_epochs\n",
    "    epoch_kl_divergence = 0\n",
    "    epoch_kl_divergence_beta = 0 \n",
    "    \n",
    "    for data in single_sample_loader:\n",
    "        data = data[0].to(torch.float)\n",
    "        # print(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, data, reduction='sum')\n",
    "        # print(reconstruction_loss.item())\n",
    "        \n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_divergence_loss_beta = beta * kl_divergence_loss\n",
    "        epoch_kl_divergence += kl_divergence_loss.item()\n",
    "        epoch_kl_divergence_beta += kl_divergence_loss_beta.item()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = reconstruction_loss + kl_divergence_loss\n",
    "        loss2 = reconstruction_loss + kl_divergence_loss_beta\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}\\nLoss (method1) = {loss.item()}\\nLoss (method2) = {loss2.item()}\")\n",
    "\n",
    "    train_loss_vals1.append(loss.item())\n",
    "    train_loss_vals2.append(loss2.item())\n",
    "\n",
    "    kl_divergences_no_beta.append(epoch_kl_divergence / len(train_loader.dataset))\n",
    "    kl_divergences_beta.append(epoch_kl_divergence_beta / len(train_loader.dataset))\n",
    "\n",
    "print(f\"Final Loss after {num_epochs} epochs: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 1000, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals1)\n",
    "plt.plot(epochs, train_loss_vals1, label='train loss (no KL annelaing)')\n",
    "plt.scatter(epochs, train_loss_vals2)\n",
    "plt.plot(epochs, train_loss_vals2, label='train loss using KL annelaing')\n",
    "plt.ylim(-10, 1000)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"train_loss_comparisons_GS.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, kl_divergences_no_beta)\n",
    "plt.plot(epochs, kl_divergences_no_beta, label='no KL annealing')\n",
    "plt.scatter(epochs, kl_divergences_beta)\n",
    "plt.plot(epochs, kl_divergences_beta, label = 'KL anneling')\n",
    "plt.xlim(0, 50)\n",
    "plt.xlabel('KL divergence')\n",
    "plt.ylabel('Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(\"kl_divergence_comparison_no_GS.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT CLIPPING PLUS SCHEDULER USED \n",
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "free_bits = 0.1\n",
    "n_epochs = 10\n",
    "\n",
    "# Model\n",
    "model = VAE_single(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Overfitting\n",
    "model.train()\n",
    "num_epochs = 1000 \n",
    "\n",
    "# Gradient clipping and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "\n",
    "# Collecting data for visualisation \n",
    "train_loss_vals1 = []\n",
    "train_loss_vals2 = []\n",
    "kl_divergences_no_beta = []\n",
    "kl_divergences_beta = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    beta = beta_start + (beta_end - beta_start) * epoch / n_epochs\n",
    "    epoch_kl_divergence = 0\n",
    "    epoch_kl_divergence_beta = 0 \n",
    "\n",
    "    for data in single_sample_loader:\n",
    "        data = data[0].to(torch.float)\n",
    "\n",
    "        # print(data)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, data, reduction='sum')\n",
    "        # print(reconstruction_loss.item())\n",
    "        \n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_divergence_loss_beta = beta * kl_divergence_loss\n",
    "        epoch_kl_divergence += kl_divergence_loss.item()\n",
    "        epoch_kl_divergence_beta += kl_divergence_loss_beta.item()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = reconstruction_loss + kl_divergence_loss\n",
    "        loss2 = reconstruction_loss + kl_divergence_loss_beta\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()  \n",
    "\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss (method1) = {loss.item()}\")\n",
    "        print(f\"Epoch {epoch}: Loss (method2) = {loss2.item()}\")\n",
    "\n",
    "    train_loss_vals1.append(loss.item())\n",
    "    train_loss_vals2.append(loss2.item())\n",
    "\n",
    "    kl_divergences_no_beta.append(epoch_kl_divergence / len(train_loader.dataset))\n",
    "    kl_divergences_beta.append(epoch_kl_divergence_beta / len(train_loader.dataset))\n",
    "\n",
    "print(f\"Final Loss after {num_epochs} epochs: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals1)\n",
    "plt.plot(epochs, train_loss_vals1, label='train loss (no KL annelaing)')\n",
    "plt.scatter(epochs, train_loss_vals2)\n",
    "plt.plot(epochs, train_loss_vals2, label='train loss using KL annelaing')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"train_loss_comparisons_GS.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, kl_divergences_no_beta)\n",
    "plt.plot(epochs, kl_divergences_no_beta, label='no KL annealing')\n",
    "plt.scatter(epochs, kl_divergences_beta)\n",
    "plt.plot(epochs, kl_divergences_beta, label = 'KL anneling')\n",
    "plt.xlim(0, 50)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL divergence value')\n",
    "plt.legend()\n",
    "plt.savefig(\"kl_divergence_comparison_GS.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Overfitting on a small train subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "free_bits = 0.1\n",
    "\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 1000  \n",
    "\n",
    "train_loss_vals1 = []\n",
    "# train_loss_vals2 = []\n",
    "kl_divergences_no_beta = []\n",
    "# kl_divergences_beta = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    beta = beta_start + (beta_end - beta_start) * epoch / num_epochs\n",
    "    epoch_kl_divergence = 0\n",
    "    for data in small_loader:\n",
    "        data = data[0].to(torch.float)\n",
    "        # print(data)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, data, reduction='sum')\n",
    "        # print(reconstruction_loss.item())\n",
    "\n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_divergence_loss_beta = beta * kl_divergence_loss\n",
    "        epoch_kl_divergence += kl_divergence_loss.item()\n",
    "        # epoch_kl_divergence_beta += kl_divergence_loss_beta.item()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = reconstruction_loss + kl_divergence_loss\n",
    "        # loss2 = reconstruction_loss + kl_divergence_loss_beta\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}\\nLoss (method1) = {loss.item()}\")\n",
    "\n",
    "    train_loss_vals1.append(loss.item())\n",
    "    # train_loss_vals2.append(loss2.item())\n",
    "\n",
    "    kl_divergences_no_beta.append(epoch_kl_divergence / len(train_loader.dataset))\n",
    "    # kl_divergences_beta.append(epoch_kl_divergence_beta / len(train_loader.dataset))\n",
    "\n",
    "print(f\"Final Loss after {num_epochs} epochs: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"saved_small_VAE1.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 1000, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals1)\n",
    "plt.plot(epochs, train_loss_vals1)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"train_loss_small_ds1.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10,8))\n",
    "# plt.scatter(epochs, kl_divergences_no_beta)\n",
    "# plt.plot(epochs, kl_divergences_no_beta, label='no KL annealing')\n",
    "# plt.scatter(epochs, kl_divergences_beta)\n",
    "# plt.plot(epochs, kl_divergences_beta, label = 'KL anneling')\n",
    "# plt.xlim(0, 50)\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('KL divergence value')\n",
    "# plt.legend()\n",
    "# plt.savefig(\"kl_divergence_comparison_GS.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "free_bits = 0.1\n",
    "\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 1000  \n",
    "\n",
    "# train_loss_vals1 = []\n",
    "train_loss_vals2 = []\n",
    "# kl_divergences_no_beta = []\n",
    "kl_divergences_beta = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    beta = beta_start + (beta_end - beta_start) * epoch / num_epochs\n",
    "    epoch_kl_divergence = 0\n",
    "    for data in small_loader:\n",
    "        data = data[0].to(torch.float)\n",
    "        # print(data)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, data, reduction='sum')\n",
    "        # print(reconstruction_loss.item())\n",
    "\n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_divergence_loss_beta = beta * kl_divergence_loss\n",
    "        # epoch_kl_divergence += kl_divergence_loss.item()\n",
    "        epoch_kl_divergence_beta += kl_divergence_loss_beta.item()\n",
    "        \n",
    "        # Total loss\n",
    "        # loss = reconstruction_loss + kl_divergence_loss\n",
    "        loss2 = reconstruction_loss + kl_divergence_loss_beta\n",
    "        \n",
    "        loss2.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}\\nLoss (method2) = {loss2.item()}\")\n",
    "\n",
    "    # train_loss_vals1.append(loss.item())\n",
    "    train_loss_vals2.append(loss2.item())\n",
    "\n",
    "    # kl_divergences_no_beta.append(epoch_kl_divergence / len(train_loader.dataset))\n",
    "    kl_divergences_beta.append(epoch_kl_divergence_beta / len(train_loader.dataset))\n",
    "\n",
    "print(f\"Final Loss after {num_epochs} epochs: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"saved_small_VAE2.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals2)\n",
    "plt.plot(epochs, train_loss_vals2)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"train_loss_small_d2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, kl_divergences_no_beta)\n",
    "plt.plot(epochs, kl_divergences_no_beta, label='no KL annealing')\n",
    "plt.scatter(epochs, kl_divergences_beta)\n",
    "plt.plot(epochs, kl_divergences_beta, label = 'KL anneling')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL divergence value')\n",
    "plt.legend()\n",
    "plt.savefig(\"kl_divergence_comparison_1_2.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) training VAE model on full dataset (train + validation sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, scheduler, n_epochs, train_loader, val_loader, beta_start, beta_end, free_bits, max_norm):\n",
    "    global train_loss_vals \n",
    "    train_loss_vals = []\n",
    "    global train_loss_vals2 \n",
    "    train_loss_vals2 = []\n",
    "    global val_loss_vals\n",
    "    val_loss_vals = []\n",
    "    train_loss = 0.0\n",
    "    train_loss2 = 0.0\n",
    "    val_loss = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    early_stopping_patience = 5\n",
    "    early_stopping_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        beta = beta_start + (beta_end - beta_start) * epoch / n_epochs\n",
    "        model.train()\n",
    "\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_train_loss2 = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            data = batch[0].to(torch.float)\n",
    "            optimizer.zero_grad()\n",
    "            recon_x, mu, logvar = model(data)\n",
    "            # print('reco_x:', recon_x[:1, :5])\n",
    "            # print('data:', data[:1, :5])\n",
    "\n",
    "            # print(recon_x.shape)\n",
    "            # print(data.shape) \n",
    "\n",
    "            reconstruction_loss = nn.functional.binary_cross_entropy(recon_x, data, reduction='sum')\n",
    "            kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            loss = reconstruction_loss + kl_divergence_loss\n",
    "            loss2 = reconstruction_loss + (beta * kl_divergence_loss)\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # Need to read more on gradient clipping \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_loss += loss.item()\n",
    "            epoch_train_loss2 += loss2.item()\n",
    "\n",
    "            # for name, param in model.named_parameters():\n",
    "            #     if param.grad is not None:\n",
    "            #         print(f'{name} gradient: {param.grad.abs().mean().item()}') \n",
    "\n",
    "        avg_train_loss = epoch_train_loss / len(train_loader.dataset)\n",
    "        avg_train_loss2 = epoch_train_loss2 / len(train_loader.dataset)\n",
    "        train_loss_vals.append(avg_train_loss)\n",
    "        train_loss_vals2.append(avg_train_loss2)\n",
    "\n",
    "        model.eval()\n",
    "        epoch_val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                data = batch[0]\n",
    "                recon_x, mu, logvar = model(data)\n",
    "                reconstruction_loss = nn.functional.binary_cross_entropy(recon_x, data, reduction='sum')\n",
    "                kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "                loss = reconstruction_loss + kl_divergence_loss\n",
    "\n",
    "                epoch_val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = epoch_val_loss / len(val_loader.dataset)\n",
    "        val_loss_vals.append(avg_val_loss)\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}:\\n\"\n",
    "                  f\" Learning Rate: {scheduler.get_last_lr()[0]}\\n\"\n",
    "                  f\" Train Loss (method 1): {avg_train_loss}\\n\"\n",
    "                  f\" Train Loss (method 2): {avg_train_loss2}\\n\"\n",
    "                  f\" Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        train_loss += avg_train_loss\n",
    "        train_loss2 += avg_train_loss2\n",
    "        val_loss += avg_val_loss\n",
    "\n",
    "        # # Check for early stopping\n",
    "        # if val_loss < best_val_loss:\n",
    "        #     best_val_loss = val_loss\n",
    "        #     early_stopping_counter = 0\n",
    "        # else:\n",
    "        #     early_stopping_counter += 1\n",
    "\n",
    "        # if early_stopping_counter >= early_stopping_patience:\n",
    "        #     print(\"Early stopping triggered\")\n",
    "        #     break\n",
    "\n",
    "    final_avg_train_loss = train_loss / n_epochs\n",
    "    final_avg_train_loss2 = train_loss2 / n_epochs\n",
    "    final_avg_val_loss = val_loss / n_epochs\n",
    "\n",
    "    print(f\"\\nFinal Average Training Loss (method 1): {final_avg_train_loss}\")\n",
    "    print(f\"Final Average Training Loss (method 2): {final_avg_train_loss2}\")\n",
    "    print(f\"Final Average Validation Loss: {final_avg_val_loss}\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaller subset of the training data\n",
    "small_subset_indices = np.random.choice(len(train_dataset), size=256, replace=False)\n",
    "small_subset = Subset(train_dataset, small_subset_indices)\n",
    "small_loader = DataLoader(small_subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "\n",
    "model1 = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-5)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "max_norm = 1.0 \n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "free_bits = 0.1\n",
    "n_epochs = 100\n",
    "\n",
    "train(model=model1, optimizer=optimizer, scheduler=scheduler, n_epochs=n_epochs, train_loader=train_loader, val_loader=val_loader, beta_start=beta_start, beta_end=beta_end, free_bits=free_bits, max_norm=max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model1.state_dict(), \"saved_base_VAE.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 100, num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loss_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals)\n",
    "plt.plot(epochs, train_loss_vals, label='Train Loss')\n",
    "plt.scatter(epochs, val_loss_vals)\n",
    "plt.plot(epochs, val_loss_vals, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"first_model_train_val_loss.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_components = 3\n",
    "# model2 = VAEWithMoGPrior(input_dim, hidden_dim, latent_dim, num_components).to(device)\n",
    "# optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# train(model=model2, optimizer=optimizer, scheduler=scheduler, n_epochs=n_epochs, train_loader=train_loader, val_loader=val_loader, beta_start=beta_start, beta_end=beta_end, free_bits=free_bits, max_norm=max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save trained model\n",
    "# torch.save(model2.state_dict(), \"saved_MoG_VAE.pt\")\n",
    "# print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observing the latent spaces of the model(s) fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract latent variables\n",
    "def get_latent_variables(model, data_loader, device):\n",
    "    model.eval()\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data,) in enumerate(data_loader):\n",
    "            data = data.to(device)\n",
    "            mean, logvar = model.encode(data)\n",
    "            latents.append(mean.cpu().numpy())\n",
    "\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "    return latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to get teh latent space\n",
    "# Get latent variables\n",
    "latents = get_latent_variables(model1, train_loader, device)\n",
    "\n",
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2)\n",
    "latents_2d = tsne.fit_transform(latents)\n",
    "\n",
    "# Plot the latent space\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(latents_2d[:, 0], latents_2d[:, 1], cmap='viridis')\n",
    "plt.colorbar(scatter)\n",
    "# plt.xlim(-400, 400)\n",
    "# plt.ylim(-400, 400)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.legend()\n",
    "plt.savefig(\"latent_space_visualisation.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying to get teh latent space\n",
    "# # Get latent variables\n",
    "# latents = get_latent_variables(model2, data_loader, device)\n",
    "\n",
    "# # Apply t-SNE for dimensionality reduction\n",
    "# tsne = TSNE(n_components=2)\n",
    "# latents_2d = tsne.fit_transform(latents)\n",
    "\n",
    "# # Plot the latent space\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# scatter = plt.scatter(latents_2d[:, 0], latents_2d[:, 1], cmap='viridis')\n",
    "# plt.colorbar(scatter)\n",
    "# # plt.xlim(-400, 400)\n",
    "# # plt.ylim(-400, 400)\n",
    "# plt.xlabel('t-SNE Dimension 1')\n",
    "# plt.ylabel('t-SNE Dimension 2')\n",
    "# plt.title('Latent Space Visualization')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Reconstruction/generation (evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model \n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "model.load_state_dict(torch.load('saved_base_VAE.pt'))  \n",
    "model.eval()  \n",
    "\n",
    "# Generate 10 new samples\n",
    "num_samples = 10 \n",
    "with torch.no_grad():\n",
    "    z = torch.randn(num_samples, latent_dim)  # Sample from the standard normal distribution (????)\n",
    "    generated_samples = model.decode(z).cpu().numpy() \n",
    "\n",
    "threshold = 0.5\n",
    "binary_generated_samples = (generated_samples > threshold).astype(float)\n",
    "\n",
    "print(\"Generated samples:\\n\", binary_generated_samples)\n",
    "print(\"Generated samples:\\n\", generated_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gridsearch best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # gridsearch\n",
    "# hidden_dim_values = [256, 512, 1024]\n",
    "# latent_dim_values = [32, 64, 128]\n",
    "# learning_rate_values = [1e-3, 1e-4, 1e-5]\n",
    "# beta_start_values = [0.01, 0.1, 0.2]\n",
    "# beta_end_values = [0.5, 1.0, 2.0]\n",
    "# free_bits_values = [0.0, 0.1, 0.2]\n",
    "# max_norm_values = [0.5, 1.0, 2.0]\n",
    "\n",
    "# # Experiment with different hyperparameter combinations\n",
    "# for hidden_dim, latent_dim, learning_rate, beta_start, beta_end, free_bits, max_norm in itertools.product(\n",
    "#     hidden_dim_values, latent_dim_values, learning_rate_values, beta_start_values, beta_end_values, free_bits_values, max_norm_values):\n",
    "#     print(f\"Training with hidden_dim={hidden_dim}, latent_dim={latent_dim}, learning_rate={learning_rate}, beta_start={beta_start}, beta_end={beta_end}, free_bits={free_bits}, max_norm={max_norm}\")\n",
    "#     model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     train1(model, optimizer, scheduler=0, n_epochs=10, train_loader=train_loader, val_loader=val_loader, beta_start=beta_start, beta_end=beta_end, free_bits=free_bits, max_norm=max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
