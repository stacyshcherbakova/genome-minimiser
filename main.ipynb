{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dir import *\n",
    "from VAE_model import *\n",
    "from VAE_model_2 import *\n",
    "from VAE_model_single import *\n",
    "from VAE_MoG_model import *\n",
    "from training import *\n",
    "from extras import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from scipy.stats import shapiro\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import itertools\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Data exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1) Overall exporation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(PANGENOME_MATRIX_CSV, index_col=[0], header=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylogroup_data = pd.read_csv(PHYLOGROUPS_DATA, index_col=[0], header=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylogroup_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.transpose()[data.transpose()[data.transpose().columns].eq(0).all(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_GF_present = data.astype(bool).sum(axis=0) / len(data.index) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_GF_present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 8))\n",
    "# percent_GF_present.iloc[:100].plot(kind='bar', color='dodgerblue')\n",
    "# plt.xlabel('Genomes')\n",
    "# plt.ylabel('Percentage of GFs present in the genome')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency1 = data.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(frequency1, color='dodgerblue')\n",
    "plt.xlabel('Gene count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.savefig(\"figures/gene_count.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency2 = data.sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(frequency2, bins=20, color='dodgerblue')\n",
    "plt.xlabel('Genome size')\n",
    "plt.ylabel('Gene Gamily Frequency')\n",
    "plt.savefig(\"figures/genome_size.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_data = []\n",
    "thresholds = np.linspace(0, 20, num=10)\n",
    "\n",
    "for i in thresholds:\n",
    "    row_sums = data.sum(axis=1)\n",
    "    threshold_data.append(len(data[row_sums >= i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(thresholds, threshold_data, color='dodgerblue')\n",
    "plt.plot(thresholds, threshold_data, color='dodgerblue')\n",
    "plt.xlabel('Gene Number Thershold')\n",
    "plt.ylabel('Gene Frequency')\n",
    "plt.savefig(\"figures/gene_frequency.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.merge(data.transpose(), phylogroup_data, how='inner', left_index=True, right_on='AccessionID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(merged_df.iloc[:, :-1])\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first two principal components\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue = merged_df.Phylogroup.tolist(), data=df_pca)\n",
    "plt.savefig(\"figures/PCA_graph.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro_test_pc1 = shapiro(df_pca['PC1'])\n",
    "shapiro_test_pc2 = shapiro(df_pca['PC2'])\n",
    "print(f\"Shapiro-Wilk Test for PC1: {shapiro_test_pc1}\")\n",
    "print(f\"Shapiro-Wilk Test for PC2: {shapiro_test_pc2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data preprocessing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = merged_df.select_dtypes(include='number')\n",
    "column_sums = numeric_cols.sum(axis=0)\n",
    "\n",
    "filtered_columns = column_sums[column_sums >= 20].index\n",
    "filtered_data = merged_df[filtered_columns]\n",
    "\n",
    "filtered_data = merged_df[filtered_columns].copy()\n",
    "filtered_data['Phylogroup'] = merged_df['Phylogroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t = np.array(filtered_data.iloc[:, :-1])\n",
    "phylogroups_array = np.array(filtered_data.iloc[:, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phylogroups_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to PyTorch tensor\n",
    "data_tensor = torch.tensor(data_array_t, dtype=torch.float32)\n",
    "\n",
    "# Spliting into train and test sets\n",
    "train_data, temp_data, train_labels, temp_labels = train_test_split(data_tensor, phylogroups_array, test_size=0.3, random_state=12345)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(temp_data, temp_labels, test_size=0.3333, random_state=12345)\n",
    "\n",
    "# train_data, val_data = train_test_split(data_tensor, test_size=0.2, random_state=12345)\n",
    "# train_data, test_data = train_test_split(data_tensor, test_size=0.25, random_state=12345)\n",
    "\n",
    "test_phylogroups = test_labels\n",
    "\n",
    "# train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "# val_labels = torch.tensor(val_labels, dtype=torch.long)\n",
    "# test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# TensorDataset\n",
    "train_dataset = TensorDataset(train_data)\n",
    "val_dataset = TensorDataset(val_data)\n",
    "test_dataset = TensorDataset(test_data)\n",
    "\n",
    "# DataLoaders for main training\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Dataloader for overfitting on one sample (for dubbiging purposes)\n",
    "input_dim = data_array_t.shape[1]\n",
    "binary_data = torch.tensor(np.random.randint(0, 2, size=(1, input_dim)), dtype=torch.float32)\n",
    "single_sample_dataset = TensorDataset(binary_data)\n",
    "single_sample_loader = DataLoader(single_sample_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "# Dataloader fot a small subset for overfitting (again, for debugging)\n",
    "small_subset_indices = np.random.choice(len(train_dataset), size=256, replace=False)\n",
    "small_subset = Subset(train_dataset, small_subset_indices)\n",
    "small_loader = DataLoader(small_subset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_phylogroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorDataset(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorDataset(torch.tensor(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Overfitting on a single sample and small data subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Overfitting on a single sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample = torch.randn(1, data_array_t.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_sample.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO GRADIENT CLIPPING AND SCHEDULER \n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "n_epochs = 10\n",
    "input_dim = data_array_t.shape[1]\n",
    "\n",
    "model = VAE_single(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Overfitting one sample to see if the model is broken \n",
    "model.train()\n",
    "num_epochs = 1000\n",
    "\n",
    "# Collecting data for visualisation \n",
    "train_loss_vals1 = []\n",
    "train_loss_vals2 = []\n",
    "kl_divergences_no_beta = []\n",
    "kl_divergences_beta = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    beta = beta_start + (beta_end - beta_start) * epoch / n_epochs\n",
    "    epoch_kl_divergence = 0\n",
    "    epoch_kl_divergence_beta = 0 \n",
    "    \n",
    "    for data in single_sample_loader:\n",
    "        data = data[0].to(torch.float)\n",
    "        # print(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, data, reduction='sum')\n",
    "        # print(reconstruction_loss.item())\n",
    "        \n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_divergence_loss_beta = beta * kl_divergence_loss\n",
    "        epoch_kl_divergence += kl_divergence_loss.item()\n",
    "        epoch_kl_divergence_beta += kl_divergence_loss_beta.item()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = reconstruction_loss + kl_divergence_loss\n",
    "        loss2 = reconstruction_loss + kl_divergence_loss_beta\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}\\nLoss (method1) = {loss.item()}\\nLoss (method2) = {loss2.item()}\")\n",
    "\n",
    "    train_loss_vals1.append(loss.item())\n",
    "    train_loss_vals2.append(loss2.item())\n",
    "\n",
    "    kl_divergences_no_beta.append(epoch_kl_divergence / len(single_sample_loader.dataset))\n",
    "    kl_divergences_beta.append(epoch_kl_divergence_beta / len(single_sample_loader.dataset))\n",
    "\n",
    "print(f\"Final Loss after {num_epochs} epochs: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/saved_single_sample_VAE_1000.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 1000, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals1, color='dodgerblue')\n",
    "plt.plot(epochs, train_loss_vals1, label='train loss (no KL annelaing)', color='dodgerblue')\n",
    "plt.scatter(epochs, train_loss_vals2, color='darkorange')\n",
    "plt.plot(epochs, train_loss_vals2, label='train loss using KL annelaing', color='darkorange')\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/train_loss_comparisons_no_GS_1000_ss.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, kl_divergences_no_beta, color='dodgerblue')\n",
    "plt.plot(epochs, kl_divergences_no_beta, label='no KL annealing', color='dodgerblue')\n",
    "plt.scatter(epochs, kl_divergences_beta, color='darkorange')\n",
    "plt.plot(epochs, kl_divergences_beta, label = 'KL anneling', color='darkorange')\n",
    "plt.xlim(0, 50)\n",
    "plt.xlabel('KL divergence')\n",
    "plt.ylabel('Epoch')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/kl_divergence_comparison_no_GS_1000_ss.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_array_t.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADIENT CLIPPING PLUS SCHEDULER USED \n",
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "n_epochs = 10\n",
    "\n",
    "# Model\n",
    "model = VAE_single(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Overfitting\n",
    "model.train()\n",
    "num_epochs = 1000 \n",
    "\n",
    "# Gradient clipping and scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Collecting data for visualisation \n",
    "train_loss_vals1 = []\n",
    "train_loss_vals2 = []\n",
    "kl_divergences_no_beta = []\n",
    "kl_divergences_beta = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    beta = beta_start + (beta_end - beta_start) * epoch / n_epochs\n",
    "    epoch_kl_divergence = 0\n",
    "    epoch_kl_divergence_beta = 0 \n",
    "\n",
    "    for data in single_sample_loader:\n",
    "        data = data[0].to(torch.float)\n",
    "\n",
    "        # print(data)\n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, data, reduction='sum')\n",
    "        # print(reconstruction_loss.item())\n",
    "        \n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_divergence_loss_beta = beta * kl_divergence_loss\n",
    "        epoch_kl_divergence += kl_divergence_loss.item()\n",
    "        epoch_kl_divergence_beta += kl_divergence_loss_beta.item()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = reconstruction_loss + kl_divergence_loss\n",
    "        loss2 = reconstruction_loss + kl_divergence_loss_beta\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()  \n",
    "\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss (method1) = {loss.item()}\")\n",
    "        print(f\"Epoch {epoch}: Loss (method2) = {loss2.item()}\")\n",
    "\n",
    "    train_loss_vals1.append(loss.item())\n",
    "    train_loss_vals2.append(loss2.item())\n",
    "\n",
    "    kl_divergences_no_beta.append(epoch_kl_divergence / len(single_sample_loader.dataset))\n",
    "    kl_divergences_beta.append(epoch_kl_divergence_beta / len(single_sample_loader.dataset))\n",
    "\n",
    "print(f\"Final Loss after {num_epochs} epochs: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals1, color='dodgerblue')\n",
    "plt.plot(epochs, train_loss_vals1, label='train loss (no KL annelaing)', color='dodgerblue')\n",
    "plt.scatter(epochs, train_loss_vals2, color='darkorange')\n",
    "plt.plot(epochs, train_loss_vals2, label='train loss using KL annelaing', color='darkorange')\n",
    "plt.xlim(0, 100)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/train_loss_comparisons_GS_1000_ss.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, kl_divergences_no_beta, color='dodgerblue')\n",
    "plt.plot(epochs, kl_divergences_no_beta, label='no KL annealing', color='dodgerblue')\n",
    "plt.scatter(epochs, kl_divergences_beta, color='darkorange')\n",
    "plt.plot(epochs, kl_divergences_beta, label = 'KL anneling', color='darkorange')\n",
    "plt.xlim(0, 50)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL divergence value')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/kl_divergence_comparison_GS_1000_ss.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Overfitting on a small train subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 1000  \n",
    "\n",
    "train_loss_vals1 = []\n",
    "# train_loss_vals2 = []\n",
    "kl_divergences_no_beta = []\n",
    "# kl_divergences_beta = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    beta = beta_start + (beta_end - beta_start) * epoch / num_epochs\n",
    "    epoch_kl_divergence = 0\n",
    "    for data in small_loader:\n",
    "        data = data[0].to(torch.float)\n",
    "        # print(data)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, data, reduction='sum')\n",
    "        # print(reconstruction_loss.item())\n",
    "\n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_divergence_loss_beta = beta * kl_divergence_loss\n",
    "        epoch_kl_divergence += kl_divergence_loss.item()\n",
    "        # epoch_kl_divergence_beta += kl_divergence_loss_beta.item()\n",
    "        \n",
    "        # Total loss\n",
    "        loss = reconstruction_loss + kl_divergence_loss\n",
    "        # loss2 = reconstruction_loss + kl_divergence_loss_beta\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}\\nLoss (method1) = {loss.item()}\")\n",
    "\n",
    "    train_loss_vals1.append(loss.item())\n",
    "    # train_loss_vals2.append(loss2.item())\n",
    "\n",
    "    kl_divergences_no_beta.append(epoch_kl_divergence / len(small_loader.dataset))\n",
    "    # kl_divergences_beta.append(epoch_kl_divergence_beta / len(small_loader.dataset))\n",
    "\n",
    "print(f\"Final Loss after {num_epochs} epochs: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"models/saved_small_VAE1_1000.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 1000, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals1, color='dodgerblue')\n",
    "plt.plot(epochs, train_loss_vals1, color='dodgerblue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/train_loss_small_ds1_1000.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, kl_divergences_no_beta, color='dodgerblue')\n",
    "plt.plot(epochs, kl_divergences_no_beta, label='no KL annealing', color='dodgerblue')\n",
    "plt.scatter(epochs, kl_divergences_beta, color='darkorange')\n",
    "plt.plot(epochs, kl_divergences_beta, label = 'KL anneling', color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL divergence value')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/kl_divergence_comparison_GS_ds1_1000.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "\n",
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "num_epochs = 1000  \n",
    "\n",
    "# train_loss_vals1 = []\n",
    "train_loss_vals2 = []\n",
    "# kl_divergences_no_beta = []\n",
    "kl_divergences_beta = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    beta = beta_start + (beta_end - beta_start) * epoch / num_epochs\n",
    "    epoch_kl_divergence_beta = 0\n",
    "    for data in small_loader:\n",
    "        data = data[0].to(torch.float)\n",
    "        # print(data)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        reconstruction, mu, logvar = model(data)\n",
    "        \n",
    "        reconstruction_loss = nn.functional.binary_cross_entropy(reconstruction, data, reduction='sum')\n",
    "        # print(reconstruction_loss.item())\n",
    "\n",
    "        kl_divergence_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        kl_divergence_loss_beta = beta * kl_divergence_loss\n",
    "        # epoch_kl_divergence += kl_divergence_loss.item()\n",
    "        epoch_kl_divergence_beta += kl_divergence_loss_beta.item()\n",
    "        \n",
    "        # Total loss\n",
    "        # loss = reconstruction_loss + kl_divergence_loss\n",
    "        loss2 = reconstruction_loss + kl_divergence_loss_beta\n",
    "        \n",
    "        loss2.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}\\nLoss (method 2) = {loss2.item()}\")\n",
    "\n",
    "    # train_loss_vals1.append(loss.item())\n",
    "    train_loss_vals2.append(loss2.item())\n",
    "\n",
    "    # kl_divergences_no_beta.append(epoch_kl_divergence / len(small_loader.dataset))\n",
    "    kl_divergences_beta.append(epoch_kl_divergence_beta / len(small_loader.dataset))\n",
    "\n",
    "print(f\"Final Loss after {num_epochs} epochs: {loss2.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model.state_dict(), \"models/saved_small_VAE2_100.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals2, color='dodgerblue')\n",
    "plt.plot(epochs, train_loss_vals2, color='dodgerblue')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/train_loss_small_ds2_1000.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, kl_divergences_no_beta, color='dodgerblue')\n",
    "plt.plot(epochs, kl_divergences_no_beta, label = 'no KL annealing', color='dodgerblue')\n",
    "plt.scatter(epochs, kl_divergences_beta, color='darkorange')\n",
    "plt.plot(epochs, kl_divergences_beta, label = 'KL anneling', color='darkorange')\n",
    "# plt.xlim(0, 1000)\n",
    "# plt.ylim(0, 20)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('KL divergence value')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/kl_divergence_comparison_1_2_1000.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Training VAE model on full dataset (train + validation sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Training with no KL annealing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "\n",
    "model1 = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "max_norm = 1.0 \n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "n_epochs = 100\n",
    "\n",
    "train_loss_vals, val_loss_vals = train_no_KL_annelaing(model=model1, optimizer=optimizer, scheduler=scheduler, n_epochs=n_epochs, train_loader=train_loader, val_loader=val_loader, max_norm=max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    if batch[0].size(0) > 0:\n",
    "        print('+') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_array_t.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model1.state_dict(), \"models/saved_no_KL_annealing_VAE_100.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 100, num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals, color='dodgerblue')\n",
    "plt.plot(epochs, train_loss_vals, label='Train Loss', color='dodgerblue')\n",
    "plt.scatter(epochs, val_loss_vals, color='darkorange')\n",
    "plt.plot(epochs, val_loss_vals, label='Validation Loss', color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/model_train_val_loss_1_100.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Training using KL annealing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "train_loss_vals2, val_loss_vals = train_with_KL_annelaing(model=model2, optimizer=optimizer, scheduler=scheduler, n_epochs=n_epochs, train_loader=train_loader, val_loader=val_loader, beta_start=beta_start, beta_end=beta_end, max_norm=max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "torch.save(model2.state_dict(), \"models/saved_KL_annealing_VAE_100.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals2, color='dodgerblue')\n",
    "plt.plot(epochs, train_loss_vals2, label='Train Loss', color='dodgerblue')\n",
    "plt.scatter(epochs, val_loss_vals, color='darkorange')\n",
    "plt.plot(epochs, val_loss_vals, label='Validation Loss', color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/model_train_val_loss_2_100.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals, color='dodgerblue')\n",
    "plt.plot(epochs, train_loss_vals, label='No KL annealing', color='dodgerblue')\n",
    "plt.scatter(epochs, train_loss_vals2, color='darkorange')\n",
    "plt.plot(epochs, train_loss_vals2, label='with KL annelaing', color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.legend()\n",
    "plt.savefig(\"figures/compare_first_second_train_losses_100.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load trained model \n",
    "\n",
    "# model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "# model.load_state_dict(torch.load('saved_KL_annealing_VAE.pt', map_location=device))\n",
    "# model.eval()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recon_x, mu, logvar = model(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) (Experiment) Training a MoG VAE (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_components = 3\n",
    "# model2 = VAEWithMoGPrior(input_dim, hidden_dim, latent_dim, num_components).to(device)\n",
    "# optimizer = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10, eta_min=0)\n",
    "# # scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=True)\n",
    "\n",
    "# train(model=model2, optimizer=optimizer, scheduler=scheduler, n_epochs=n_epochs, train_loader=train_loader, val_loader=val_loader, beta_start=beta_start, beta_end=beta_end, max_norm=max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save trained model\n",
    "# torch.save(model2.state_dict(), \"models/saved_MoG_VAE.pt\")\n",
    "# print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Observing the latent spaces of the model(s) fitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 512\n",
    "latent_dim = 64\n",
    "input_dim = data_array_t.shape[1]\n",
    "\n",
    "# Trying to get teh latent space\n",
    "model2 = VAE(input_dim, hidden_dim, latent_dim)\n",
    "model2.load_state_dict(torch.load('models/saved_KL_annealing_VAE_100.pt'))  \n",
    "model2.eval()  \n",
    "\n",
    "# Get latent variables\n",
    "latents = get_latent_variables(model2, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2)\n",
    "tsne_latents = tsne.fit_transform(latents)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(tsne_latents[:, 0], tsne_latents[:, 1], color='dodgerblue')\n",
    "# plt.xlim(-400, 400)\n",
    "# plt.ylim(-400, 400)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(tsne_latents, columns=['PC1', 'PC2'])\n",
    "df_tsne['phylogroup'] = test_phylogroups\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue = df_tsne['phylogroup'], data=df_tsne)\n",
    "plt.savefig(\"figures/tsne_latent_space_visualisation.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(latents)\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['phylogroup'] = test_phylogroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA results\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue = df_pca['phylogroup'], data=df_pca)\n",
    "plt.savefig(\"figures/pca_latent_space_visualisation.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1) Gridserch for simple hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Gridsearch\n",
    "# input_dim = data_array_t.shape[1]\n",
    "# hidden_dim_values = [256, 512, 1024]\n",
    "# latent_dim_values = [32, 64, 128]\n",
    "# learning_rate_values = [0.01, 1e-3] # Decrease of learning rate causes higher average train loss, better if 0.01, 0.001\n",
    "# # beta_start_values = [0.01, 0.1, 0.2]\n",
    "# # beta_end_values = [0.5, 1.0, 2.0]\n",
    "# # max_norm_values = [0.5, 1.0, 2.0]\n",
    "# max_norm = 1.0 \n",
    "# beta_start = 0.1\n",
    "# beta_end = 1.0\n",
    "\n",
    "# # beta_start, beta_end, max_norm\n",
    "# for hidden_dim, latent_dim, learning_rate in itertools.product(\n",
    "#     hidden_dim_values, latent_dim_values, learning_rate_values): #beta_start_values, beta_end_values, max_norm_values\n",
    "#     print(f\"Training with hidden_dim={hidden_dim}, latent_dim={latent_dim}, learning_rate={learning_rate}\") # beta_start={beta_start}, beta_end={beta_end}, max_norm={max_norm}\"\n",
    "#     model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#     scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "#     train_with_KL_annelaing(model=model, optimizer=optimizer, scheduler=scheduler, n_epochs=10, train_loader=train_loader, val_loader=val_loader, beta_start=beta_start, beta_end=beta_end, max_norm=max_norm)\n",
    "#     print(\"--------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result - best params hidden_dim = 1024, latent_dim = 32, lr = 1e-3 (based on average train and val loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = data_array_t.shape[1]\n",
    "hidden_dim = 1024\n",
    "latent_dim = 32\n",
    "max_norm = 1.0 \n",
    "beta_start = 0.1\n",
    "beta_end = 1.0\n",
    "n_epochs = 100\n",
    "\n",
    "\n",
    "model = VAE(input_dim, hidden_dim, latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "train_loss_vals2, val_loss_vals = train_with_KL_annelaing(model=model, optimizer=optimizer, scheduler=scheduler, n_epochs=n_epochs, train_loader=train_loader, val_loader=val_loader, beta_start=beta_start, beta_end=beta_end, max_norm=max_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/saved_KL_annealing_VAE_tuned_100.pt\")\n",
    "print(\"Model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.scatter(epochs, train_loss_vals2, color='dodgerblue')\n",
    "plt.plot(epochs, train_loss_vals2, label='Train Loss', color='dodgerblue')\n",
    "plt.scatter(epochs, val_loss_vals, color='darkorange')\n",
    "plt.plot(epochs, val_loss_vals, label='Validation Loss', color='darkorange')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "# AHPT - after hyperparameter tuning\n",
    "plt.savefig(\"figures/train_val_loss_AHPT_100.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the first two principal components\n",
    "latents = get_latent_variables(model, test_loader, device)\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(latents)\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['phylogroup'] = test_phylogroups\n",
    "\n",
    "df_tsne = pd.DataFrame(tsne_latents, columns=['PC1', 'PC2'])\n",
    "df_tsne['phylogroup'] = test_phylogroups\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue = df_tsne['phylogroup'], data=df_tsne)\n",
    "plt.savefig(\"figures/pca_latent_space_visualisation_AHPT.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE for dimensionality reduction\n",
    "tsne = TSNE(n_components=2)\n",
    "data_tsne = tsne.fit_transform(latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tsne = pd.DataFrame(tsne_latents, columns=['PC1', 'PC2'])\n",
    "df_tsne['phylogroup'] = test_phylogroups\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue = df_tsne['phylogroup'], data=df_tsne)\n",
    "plt.savefig(\"figures/tsne_latent_space_visualisation.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_tsne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Evaluation test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(input_dim, hidden_dim, latent_dim)\n",
    "model.load_state_dict(torch.load('models/saved_KL_annealing_VAE_tuned_100.pt'))  \n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon_x, mu, logvar = model(test_data)\n",
    "\n",
    "recon_x_binarized = (recon_x > 0.5).int()\n",
    "\n",
    "f1 = sklearn.metrics.f1_score(test_data.flatten(), recon_x_binarized.flatten())\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(test_data.flatten(), recon_x_binarized.flatten())\n",
    "print(f'Accuracy Score: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_x_binarized = (recon_x > 0.5).int()\n",
    "\n",
    "f1_scores = []\n",
    "accuracy_scores = []\n",
    "for genome_x, genome in zip(recon_x_binarized, test_data):\n",
    "    f1_scores.append(sklearn.metrics.f1_score(genome_x, genome))\n",
    "    accuracy_scores.append(sklearn.metrics.accuracy_score(genome_x, genome))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(f1_scores, color='dodgerblue')\n",
    "plt.xlabel(\"F1 score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"figures/f1_score_frequency_test_set_AHPT.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,8))\n",
    "plt.hist(accuracy_scores, color='dodgerblue')\n",
    "plt.xlabel(\"Accuracy score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.savefig(\"figures/accuracy_score_frequency_test_set_AHPT.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Simulation/generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['encoder.0.weight', 'encoder.0.bias', 'encoder.1.weight', 'encoder.1.bias', 'encoder.1.running_mean', 'encoder.1.running_var', 'encoder.1.num_batches_tracked', 'encoder.3.weight', 'encoder.3.bias', 'encoder.4.weight', 'encoder.4.bias', 'encoder.4.running_mean', 'encoder.4.running_var', 'encoder.4.num_batches_tracked', 'encoder.6.weight', 'encoder.6.bias', 'encoder.7.weight', 'encoder.7.bias', 'encoder.7.running_mean', 'encoder.7.running_var', 'encoder.7.num_batches_tracked', 'mean_layer.weight', 'mean_layer.bias', 'logvar_layer.weight', 'logvar_layer.bias', 'decoder.0.weight', 'decoder.0.bias', 'decoder.1.weight', 'decoder.1.bias', 'decoder.1.running_mean', 'decoder.1.running_var', 'decoder.1.num_batches_tracked', 'decoder.3.weight', 'decoder.3.bias', 'decoder.4.weight', 'decoder.4.bias', 'decoder.4.running_mean', 'decoder.4.running_var', 'decoder.4.num_batches_tracked', 'decoder.6.weight', 'decoder.6.bias', 'decoder.7.weight', 'decoder.7.bias', 'decoder.7.running_mean', 'decoder.7.running_var', 'decoder.7.num_batches_tracked', 'decoder.9.weight', 'decoder.9.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('models/saved_KL_annealing_VAE_BD_100_AHPT.pt', map_location=torch.device('cpu'))\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1) Random sampling from latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples (binary):\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "Generated samples (sigmoid function output):\n",
      " [[1.7769073e-04 3.2114153e-07 1.8784209e-01 ... 7.4324407e-02\n",
      "  1.7745629e-01 2.3535958e-05]\n",
      " [5.1390733e-05 1.0870822e-09 7.5103067e-02 ... 5.4485691e-03\n",
      "  1.2381562e-02 1.4658183e-08]\n",
      " [2.2642735e-02 4.5095883e-02 4.3876082e-02 ... 9.1393203e-02\n",
      "  2.0216005e-02 4.3009086e-06]\n",
      " ...\n",
      " [2.4069037e-02 1.5522110e-01 3.4874178e-02 ... 6.6571529e-03\n",
      "  5.5042654e-03 4.7373556e-04]\n",
      " [5.3547625e-04 2.4078868e-02 3.4662257e-03 ... 8.0089215e-03\n",
      "  4.9852736e-02 7.6145463e-04]\n",
      " [1.2590332e-03 5.8197655e-02 2.8905066e-02 ... 3.6031518e-02\n",
      "  2.8072030e-03 1.9860319e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Load trained model \n",
    "input_dim = 7580\n",
    "hidden_dim = 512\n",
    "latent_dim = 32\n",
    "\n",
    "# changes layer norm layer to batch norm layer and \n",
    "model = VAE_2(input_dim, hidden_dim, latent_dim)\n",
    "model.load_state_dict(torch.load('models/saved_KL_annealing_VAE_BD_100_AHPT.pt',  map_location=torch.device('cpu')))  \n",
    "model.eval()  \n",
    "\n",
    "# Generate 10 new samples\n",
    "num_samples = 10 \n",
    "with torch.no_grad():\n",
    "    z = torch.randn(num_samples, latent_dim)  # Sample from the standard normal distribution because the latent space follows normal distribution \n",
    "    generated_samples = model.decode(z).cpu().numpy() \n",
    "\n",
    "threshold = 0.5\n",
    "binary_generated_samples = (generated_samples > threshold).astype(float)\n",
    "\n",
    "print(\"Generated samples (binary):\\n\", binary_generated_samples)\n",
    "print(\"\\n\")\n",
    "print(\"Generated samples (sigmoid function output):\\n\", generated_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2) Grid sampling from latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(latents)\n",
    "df_pca = pd.DataFrame(data_pca, columns=['PC1', 'PC2'])\n",
    "df_pca['phylogroup'] = test_phylogroups\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PCA results\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue = df_pca['phylogroup'], data=df_pca)\n",
    "plt.savefig(\"figures/pca_latent_space_visualisation.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size = 3\n",
    "scale = 21.0\n",
    "\n",
    "x = np.linspace(-scale, scale, grid_size)\n",
    "y = np.linspace(-scale, scale, grid_size)\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "grid_points = np.stack([xx.ravel(), yy.ravel()], axis=-1)\n",
    "grid_points = torch.tensor(grid_points, dtype=torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2., -2.],\n",
       "        [ 0., -2.],\n",
       "        [ 2., -2.],\n",
       "        [-2.,  0.],\n",
       "        [ 0.,  0.],\n",
       "        [ 2.,  0.],\n",
       "        [-2.,  2.],\n",
       "        [ 0.,  2.],\n",
       "        [ 2.,  2.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_high_dim_grid_points = pca.inverse_transform(grid_points)\n",
    "\n",
    "new_high_dim_grid_points = torch.tensor(new_high_dim_grid_points, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_samples = model.decode(new_high_dim_grid_points).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated samples (binary):\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "\n",
      "Generated samples (sigmoid function output):\n",
      " [[7.7252116e-05 1.2633001e-03 4.7773786e-02 ... 1.7216282e-01\n",
      "  2.2460228e-01 2.9453573e-05]\n",
      " [6.8355046e-05 1.4295902e-03 5.1858902e-02 ... 1.6925547e-01\n",
      "  2.2957756e-01 5.6759844e-05]\n",
      " [2.0140858e-04 3.8045559e-03 9.3354106e-02 ... 1.7241058e-01\n",
      "  1.9027841e-01 1.2891165e-03]\n",
      " ...\n",
      " [7.9948673e-05 2.4576401e-03 5.0572816e-02 ... 1.5987152e-01\n",
      "  2.2472443e-01 7.1877468e-05]\n",
      " [1.0823663e-04 1.2167287e-03 5.3681433e-02 ... 1.6496356e-01\n",
      "  2.1537539e-01 1.2699362e-04]\n",
      " [3.2322315e-04 3.8754996e-03 1.0536576e-01 ... 1.8948840e-01\n",
      "  1.8244295e-01 2.4207111e-03]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "binary_generated_samples = (generated_samples > threshold).astype(float)\n",
    "\n",
    "print(\"Generated samples (binary):\\n\", binary_generated_samples)\n",
    "print(\"\\n\")\n",
    "print(\"Generated samples (sigmoid function output):\\n\", generated_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3) Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DGV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
